### 神经网络基础

#### 为什么需要激活函数？

从简单的前馈神经网络说起，一个最简单的线性模型为：$f(x; w, b) = x^Tw + b$，这个模型是无法解决XOR问题的，即使加一个隐藏层，依然无法解决此问题，因为两层前馈神经网络依然是线性模型，为了使模型具有非线性的性质，在隐层中加入激活函数，可以实现非线性的变换，常见的激活函数有ReLU, sigmoid, gelu等等。

#### 基于梯度的学习
由于神经网络模型的非线性性质，导致损失函数大多为非凸函数，无法像线性模型那样，直接通过方程求解。因此只能通过迭代，基于梯度进行优化。

#### 损失函数

大多数情况下，参数模型定义为一个分布：$p(y|x;\theta)$, 并使用最大似然原理，因此一般都使用交叉熵作为损失函数。

损失函数一般是求最小值，所以损失函数为负的最大对数似然，其等价于训练数据和模型分布间的交叉熵。损失函数为：
$$J(\theta) = -E_{x,y ~ p^-_{data}}log p_{model}(y|x)$$

##### 最大似然和交叉熵

最大似然和交叉熵来自于不同的思路，但是最终的结果是等价的。

* 最大似然

假设$p_{model}(x;\theta)$为模型分布，模型会将任意输入$x$映射到实数来估计真实概率$p_{data}(x)$。

对$\theta$的最大似然估计被定义为：
$$\theta_{ML} = arg_\theta max  p_{model}(X;\theta)$$

$$=arg_{\theta}max\prod_{i=1}^mp_{model}(x^{(i)};\theta)$$

连乘不好算，所以等价为求和方式：
$$\theta_{ML} = arg_{\theta}max\sum_{i=1}^mlog p_{model}(x^{(i)};\theta)$$

因为重新缩放损失函数时，argmax不会改变，所以除以m得到和训练数据经验分布$p_{data}^{-}$相关的期望作为准则：
$$\theta_{ML} = arg_{\theta}minE_{X～p_{data}^{-}}log p_{model}(x^{(i)};\theta)$$

* 交叉熵

交叉熵损失函数就是用熵的概念来表示损失。
             
